{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sawsan_Awad_300327224_DSA_Assignment(3)_RL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Install Packages"
      ],
      "metadata": {
        "id": "bfGx6jQsmC9N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "C8K8ZMFn14IN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "656ed82e-ae8f-45c3-b74e-54e0190e57c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.7/dist-packages (3.22.4)\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy) (1.21.6)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.3.0)\n",
            "Requirement already satisfied: atari-py~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (0.2.9)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (7.1.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (4.1.2.30)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py~=0.2.0->gym[atari]) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install cmake 'gym[atari]' scipy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importing Libraries"
      ],
      "metadata": {
        "id": "EyRU5i5emiUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym \n",
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "import random\n",
        "from IPython.display import clear_output\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "R2Vyc8I8mWg7"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up the environment"
      ],
      "metadata": {
        "id": "0yyvqym9ULB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def agent_enviro (state):\n",
        "  env = gym.make(\"Taxi-v3\").env\n",
        "  env.render()\n",
        "  env.reset() # reset environment to a new, random state\n",
        "  env.s = state\n",
        "  env.render()\n",
        "  print(\"Action Space {}\".format(env.action_space))\n",
        "  print(\"State Space {}\".format(env.observation_space))\n",
        "  return state,env"
      ],
      "metadata": {
        "id": "6ZZnRyk9G2Qm"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state,env = agent_enviro(414)\n",
        "state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SvjuTa8Jdn_",
        "outputId": "0fad0576-3350-4479-e1d4-6583df5f4c3f"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : :\u001b[43m \u001b[0m|\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "\n",
            "Action Space Discrete(6)\n",
            "State Space Discrete(500)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "414"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-learning approach"
      ],
      "metadata": {
        "id": "mIR_WnoWUtI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Train_the_agent(game, alpha,gamma,epsilon):\n",
        "  %%time\n",
        "  \"\"\"Training the agent\"\"\"\n",
        "  env = gym.make(game).env\n",
        "  env.reset()\n",
        "  env.render()\n",
        "  decay_factor=1e-3\n",
        "\n",
        "  # Initialize the q table\n",
        "  q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "  # Initialize the q table\n",
        "  q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "\n",
        "  # Hyperparameters\n",
        "  #alpha = 0.1\n",
        "  #gamma = 0.6\n",
        "  #epsilon = 0.1\n",
        "\n",
        "  # For plotting metrics\n",
        "  all_epochs = []\n",
        "  all_penalties = []\n",
        "\n",
        "  for i in range(1, 100001):\n",
        "      state = env.reset()\n",
        "\n",
        "      epochs, penalties, reward, = 0, 0, 0\n",
        "      done = False\n",
        "      if i % 10000==0:\n",
        "        epsilon-=decay_factor\n",
        "      \n",
        "      while not done:\n",
        "          if random.uniform(0, 1) < epsilon:\n",
        "              action = env.action_space.sample() # Explore action space\n",
        "          else:\n",
        "              action = np.argmax(q_table[state]) # Exploit learned values\n",
        "\n",
        "          next_state, reward, done, info = env.step(action) \n",
        "          \n",
        "          old_value = q_table[state, action]\n",
        "          next_max = np.max(q_table[next_state])\n",
        "      \n",
        "          new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
        "          q_table[state, action] = new_value\n",
        "\n",
        "          if reward == -10:\n",
        "              penalties += 1\n",
        "\n",
        "          state = next_state\n",
        "          epochs += 1\n",
        "          \n",
        "      if i % 100 == 0:\n",
        "          clear_output(wait=True)\n",
        "          print(f\"Episode: {i}\")\n",
        "\n",
        "  print(\"Training finished.\\n\")\n",
        "  return q_table"
      ],
      "metadata": {
        "id": "spjZSoYqibrT"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_table = Train_the_agent(\"Taxi-v3\",0.1,0.6,0.1)\n",
        "print(q_table)"
      ],
      "metadata": {
        "id": "WXjr2PRTQzq3",
        "outputId": "66556c06-e15c-49d2-b225-7369e0a82c34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 100000\n",
            "Training finished.\n",
            "\n",
            "[[  0.           0.           0.           0.           0.\n",
            "    0.        ]\n",
            " [ -2.41837056  -2.36395107  -2.41837034  -2.3639511   -2.27325184\n",
            "  -11.3638796 ]\n",
            " [ -1.87014387  -1.45024021  -1.87014398  -1.45024012  -0.7504\n",
            "  -10.45023915]\n",
            " ...\n",
            " [ -0.97647143   0.41599899  -1.02677684  -1.07474385  -3.44152954\n",
            "   -3.44457388]\n",
            " [ -2.15651358  -2.12203237  -2.14510113  -2.12203606  -5.76585125\n",
            "   -5.66999129]\n",
            " [  0.90524      1.09096448   2.19711119  11.          -1.98282408\n",
            "   -1.98284877]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evalutation"
      ],
      "metadata": {
        "id": "IZ_rv1Z0U2Yx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
        "def Evaluation(q_table):\n",
        "  total_epochs, total_penalties = 0, 0\n",
        "  episodes = 1000\n",
        "\n",
        "  for _ in range(episodes):\n",
        "      state = env.reset()\n",
        "      epochs, penalties, reward = 0, 0, 0\n",
        "      \n",
        "      done = False\n",
        "      \n",
        "      while not done:\n",
        "          action = np.argmax(q_table[state])\n",
        "          state, reward, done, info = env.step(action)\n",
        "\n",
        "          if reward == -10:\n",
        "              penalties += 1\n",
        "\n",
        "          epochs += 1\n",
        "\n",
        "      total_penalties += penalties\n",
        "      total_epochs += epochs\n",
        "\n",
        "  print(f\"Results after {episodes} episodes:\")\n",
        "  print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
        "  print(f\"Average penalties per episode: {total_penalties / episodes}\")"
      ],
      "metadata": {
        "id": "XXM_lpN_PMj0"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Evaluation(q_table)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XJro2LpKmm_",
        "outputId": "049dade6-f0e9-4dea-af7f-b96e074eccd0"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results after 1000 episodes:\n",
            "Average timesteps per episode: 13.186\n",
            "Average penalties per episode: 0.0\n"
          ]
        }
      ]
    }
  ]
}